# Bayesian Benchmarks

Tools for evaluating Bayesian models.

Problems this repository attempts to solve:
* Variations between tasks in the literature make a fair comparison between methods difficult.
* Implementing competing methods takes considerable effort, and there is an obvious incentive to do a poor job.
* Published papers may not always provide complete details of implementations due to space considerations.
* There is a lack of standardized tasks that meaningfully assess the quality of uncertainty quantification.


Key aims (science):
* Curate a set of benchmarks that meaningfully compare the efficacy of Bayesian models in real-world tasks.
* Maintain a fair assessment of benchmark methods, with full implementations and results.

Key aims (code):
* Transparancy
* Easy of use
* Reproducibility

Current tasks:
* Classification and regression.
* Density estimation (real world and synthetic)
* Active learning
* Adversarial robustness
