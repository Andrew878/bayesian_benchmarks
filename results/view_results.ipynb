{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import sys\n",
    "sys.path.append('../tasks')\n",
    "from database_utils import Database\n",
    "\n",
    "from data import ALL_REGRESSION_DATATSETS, ALL_CLASSIFICATION_DATATSETS\n",
    "ALL_DATATSETS = {}\n",
    "ALL_DATATSETS.update(ALL_REGRESSION_DATATSETS)\n",
    "ALL_DATATSETS.update(ALL_CLASSIFICATION_DATATSETS)\n",
    "\n",
    "regression_datasets = list(ALL_REGRESSION_DATATSETS.keys())\n",
    "regression_datasets.sort()\n",
    "\n",
    "classification_datasets = list(ALL_CLASSIFICATION_DATATSETS.keys())\n",
    "classification_datasets.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankarray(A):\n",
    "    ranks = []\n",
    "    for a in A:\n",
    "        ranks.append(rankdata(a))\n",
    "    return np.array(ranks)\n",
    "\n",
    "\n",
    "def read_regression_classification(fs, models_names, datasets, task):\n",
    "    if task == 'classification':\n",
    "        fields = ['dataset', 'N', 'D', 'K'] + [m[1] for m in models_names]\n",
    "    else:\n",
    "        fields = ['dataset', 'N', 'D'] + [m[1] for m in models_names]\n",
    "\n",
    "    results = {}\n",
    "    for f in fs:\n",
    "        results[f] = {'table':{f:[] for f in fields}, 'vals':[]}\n",
    "\n",
    "    with Database('../results/results.db') as db:\n",
    "\n",
    "        for dataset in datasets:\n",
    "\n",
    "            for f in fs:\n",
    "                results[f]['table']['dataset'].append(dataset[:10])\n",
    "                results[f]['table']['N'].append(ALL_DATATSETS[dataset].N)\n",
    "                results[f]['table']['D'].append(ALL_DATATSETS[dataset].D)\n",
    "                if task == 'classification':\n",
    "                    results[f]['table']['K'].append(ALL_DATATSETS[dataset].K)\n",
    "\n",
    "            row = {f:[] for f in fs}\n",
    "            for model, name in models_names:\n",
    "\n",
    "                res = db.read(task, fs, {'model':model, 'dataset':dataset})\n",
    "\n",
    "                if len(res) == 0:\n",
    "                    for f in fs:\n",
    "                        results[f]['table'][name].append('')\n",
    "                        row[f].append(np.nan)\n",
    "                else:\n",
    "                    for i, f in enumerate(fs):\n",
    "                        L = [float(l[i]) for l in res]\n",
    "                        m = np.average(L)\n",
    "                        if m < 1000 and m > -1000:\n",
    "                            r = '{:.3f}'.format(m)\n",
    "                            row[f].append(m)\n",
    "                        else:\n",
    "                            r = 'nan'\n",
    "                            row[f].append(np.nan)\n",
    "\n",
    "                        results[f]['table'][name].append(r)\n",
    "\n",
    "            #             stderr = np.std(L)/float(len(L))**0.5\n",
    "            #             r = '{:.3f} ({:.3f})'.format(m, stderr)\n",
    "            for f in fs:   \n",
    "                results[f]['vals'].append(row[f])\n",
    "\n",
    "\n",
    "    for f in fs:\n",
    "        if 'unnormalized' not in f:\n",
    "            vals = np.array(results[f]['vals'])\n",
    "\n",
    "            avgs = np.nanmean(vals, 0)\n",
    "            meds = np.nanmedian(vals, 0)\n",
    "            rks = np.nanmean(rankarray(vals), 0)\n",
    "\n",
    "            for s, n in [[avgs, 'avg'], [meds, 'median'], [rks, 'avg rank']]:\n",
    "                results[f]['table']['dataset'].append(n)\n",
    "                results[f]['table']['N'].append('')\n",
    "                results[f]['table']['D'].append('')\n",
    "                if task == 'classification':\n",
    "                    results[f]['table']['K'].append('')\n",
    "                for ss, name in zip(s, [m[1] for m in models_names]):\n",
    "                    results[f]['table'][name].append('{:.3f}'.format(ss))\n",
    "    \n",
    "    return results, fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = [['linear', 'lin'],\n",
    "                ['variationally_sparse_gp', 'SVGP'],\n",
    "                ['deep_gp_doubly_stochastic','DGP'],\n",
    "                ['svm', 'svm'],\n",
    "                ['knn', 'knn'],\n",
    "#                 ['naive_bayes', 'nb'],\n",
    "#                 ['decision_tree', 'dt'],\n",
    "#                 ['random_forest', 'rf'],\n",
    "                ['gradient_boosting_machine', 'gbm'],\n",
    "#                 ['adaboost', 'ab'],\n",
    "                ['mlp', 'mlp'],\n",
    "                ]\n",
    "\n",
    "fs = 'test_loglik', 'test_rmse', 'test_loglik_unnormalized', 'test_rmse_unnormalized'\n",
    "\n",
    "results, fields = read_regression_classification(fs, models_names, regression_datasets, 'regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "   dataset &      N &   D &     lin &    SVGP &     DGP &     svm &     knn &     gbm &     mlp \\\\\n",
      "\\midrule\n",
      "    boston &    506 &  13 &  -0.545 &  -0.089 &  -0.160 &  -0.107 &  -0.296 &  -0.516 &  -0.123 \\\\\n",
      "  concrete &   1030 &   8 &  -0.965 &  -0.387 &  -0.438 &  -0.716 &  -0.875 &  -0.421 &  -0.636 \\\\\n",
      "    energy &    768 &   8 &  -0.074 &   1.205 &   1.216 &   0.135 &   0.218 &   1.746 &   0.071 \\\\\n",
      "    kin8nm &   8192 &   8 &  -1.118 &  -0.355 &  -0.079 &  -0.245 &  -0.653 &  -1.002 &  -0.265 \\\\\n",
      "     naval &  11934 &  12 &  -0.454 &   2.214 &   1.476 &   0.048 &   0.870 &  -0.092 &   1.283 \\\\\n",
      "     power &   9568 &   4 &  -0.140 &  -0.029 &  -0.109 &  -0.038 &  -0.057 &  -0.027 &  -0.059 \\\\\n",
      "   protein &  45730 &   9 &  -1.248 &  -1.107 &  -1.612 &  -1.131 &  -1.009 &  -1.149 &  -1.087 \\\\\n",
      "   winered &   1599 &  11 &  -1.134 &  -1.071 &  -1.083 &  -1.066 &  -1.206 &  -1.077 &  -1.095 \\\\\n",
      " winewhite &   4898 &  12 &  -1.215 &  -1.136 &         &  -1.147 &  -1.160 &  -1.146 &  -1.136 \\\\\n",
      "     yacht &    308 &   6 &  -0.987 &   1.943 &   0.385 &  -1.024 &  -1.559 &  -3.981 &  -0.457 \\\\\n",
      "       avg &        &     &  -0.788 &   0.119 &  -0.045 &  -0.529 &  -0.573 &  -0.766 &  -0.350 \\\\\n",
      "    median &        &     &  -0.976 &  -0.222 &  -0.109 &  -0.481 &  -0.764 &  -0.759 &  -0.361 \\\\\n",
      "  avg rank &        &     &   1.500 &   5.900 &   4.800 &   4.300 &   3.200 &   3.900 &   4.400 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "   dataset &      N &   D &    lin &   SVGP &    DGP &    svm &    knn &    gbm &    mlp \\\\\n",
      "\\midrule\n",
      "    boston &    506 &  13 &  0.372 &  0.239 &  0.248 &  0.246 &  0.300 &  0.278 &  0.269 \\\\\n",
      "  concrete &   1030 &   8 &  0.635 &  0.375 &  0.386 &  0.448 &  0.533 &  0.320 &  0.392 \\\\\n",
      "    energy &    768 &   8 &  0.256 &  0.061 &  0.056 &  0.211 &  0.189 &  0.042 &  0.224 \\\\\n",
      "    kin8nm &   8192 &   8 &  0.739 &  0.327 &  0.257 &  0.306 &  0.446 &  0.656 &  0.310 \\\\\n",
      "     naval &  11934 &  12 &  0.380 &  0.017 &  0.020 &  0.230 &  0.096 &  0.265 &  0.067 \\\\\n",
      "     power &   9568 &   4 &  0.278 &  0.248 &  0.248 &  0.250 &  0.232 &  0.245 &  0.255 \\\\\n",
      "   protein &  45730 &   9 &  0.843 &  0.732 &  0.709 &  0.750 &  0.622 &  0.764 &  0.717 \\\\\n",
      "   winered &   1599 &  11 &  0.748 &  0.704 &  0.709 &  0.703 &  0.787 &  0.698 &  0.722 \\\\\n",
      " winewhite &   4898 &  12 &  0.814 &  0.751 &        &  0.760 &  0.753 &  0.759 &  0.753 \\\\\n",
      "     yacht &    308 &   6 &  0.642 &  0.052 &  0.162 &  0.542 &  0.832 &  0.063 &  0.312 \\\\\n",
      "       avg &        &     &  0.571 &  0.351 &  0.311 &  0.444 &  0.479 &  0.409 &  0.402 \\\\\n",
      "    median &        &     &  0.638 &  0.288 &  0.248 &  0.377 &  0.489 &  0.299 &  0.311 \\\\\n",
      "  avg rank &        &     &  6.700 &  2.400 &  3.000 &  4.100 &  4.300 &  3.400 &  4.100 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(results['test_loglik']['table'], columns=fields).to_latex(index=False))\n",
    "print(pandas.DataFrame(results['test_rmse']['table'], columns=fields).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = ['test_loglik', 'test_acc']\n",
    "results, fields = read_regression_classification(fs, models_names, classification_datasets, 'classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllll}\n",
      "\\toprule\n",
      "    dataset &       N &    D &    K &     lin &    SVGP &     DGP &     svm &     knn &     gbm &     mlp \\\\\n",
      "\\midrule\n",
      "    abalone &    4177 &    9 &    3 &  -0.754 &         &         &  -0.730 &  -2.169 &  -0.654 &  -0.692 \\\\\n",
      " acute-infl &     120 &    7 &    2 &  -0.069 &  -0.009 &  -0.119 &  -0.019 &  -0.000 &  -0.000 &  -0.033 \\\\\n",
      " acute-neph &     120 &    7 &    2 &  -0.032 &  -0.005 &  -0.084 &  -0.020 &  -0.000 &  -0.000 &  -0.020 \\\\\n",
      "      adult &   48842 &   15 &    2 &  -0.348 &  -0.321 &         &  -0.363 &  -1.109 &  -0.295 &  -0.318 \\\\\n",
      "  annealing &     898 &   32 &    5 &  -0.479 &  -1.645 &         &  -0.509 &  -1.197 &  -0.134 &  -0.402 \\\\\n",
      " arrhythmia &     452 &  263 &   13 &  -1.369 &  -1.389 &         &  -1.213 &  -7.528 &  -1.612 &  -1.256 \\\\\n",
      " audiology- &     196 &   60 &   18 &  -1.181 &  -1.073 &  -1.218 &  -1.167 &  -2.101 &  -0.849 &  -0.753 \\\\\n",
      " balance-sc &     625 &    5 &    3 &  -0.178 &  -0.035 &  -0.057 &  -0.176 &  -0.652 &  -0.270 &  -0.083 \\\\\n",
      "   balloons &      16 &    5 &    2 &  -1.341 &         &  -0.868 &  -0.885 &  -0.714 &  -6.096 &  -1.743 \\\\\n",
      "       bank &    4521 &   17 &    2 &  -0.259 &  -0.248 &         &  -0.277 &  -1.287 &  -0.230 &  -0.296 \\\\\n",
      "      blood &     748 &    5 &    2 &  -0.510 &  -0.509 &  -0.493 &  -0.539 &  -1.854 &  -0.535 &  -0.485 \\\\\n",
      " breast-can &     286 &   10 &    2 &  -0.182 &  -0.120 &  -0.190 &  -0.176 &  -0.406 &  -0.000 &  -0.156 \\\\\n",
      " breast-can &     699 &   10 &    2 &  -0.052 &  -0.051 &  -0.053 &  -0.043 &  -0.416 &  -0.033 &  -0.026 \\\\\n",
      " breast-can &     569 &   31 &    2 &  -0.043 &  -0.037 &  -0.070 &  -0.058 &  -0.100 &  -0.036 &  -0.031 \\\\\n",
      " breast-can &     198 &   34 &    2 &  -0.925 &  -0.743 &  -0.686 &  -0.760 &  -1.878 &  -1.007 &  -0.856 \\\\\n",
      " breast-tis &     106 &   10 &    6 &  -0.754 &         &  -0.240 &  -0.675 &  -0.409 &  -1.647 &  -0.392 \\\\\n",
      "        car &    1728 &    7 &    4 &  -0.457 &  -0.017 &  -0.017 &  -0.064 &  -0.193 &  -0.073 &  -0.063 \\\\\n",
      " cardiotoco &    2126 &   22 &   10 &  -0.705 &         &         &  -0.500 &  -1.750 &  -0.454 &  -0.454 \\\\\n",
      " cardiotoco &    2126 &   22 &    3 &  -0.267 &  -0.410 &         &  -0.241 &  -0.545 &  -0.146 &  -0.309 \\\\\n",
      " chess-krvk &   28056 &    7 &   18 &  -1.369 &         &         &  -0.088 &  -0.545 &  -0.001 &  -0.029 \\\\\n",
      " chess-krvk &    3196 &   37 &    2 &  -0.060 &  -0.033 &         &  -0.020 &  -0.223 &  -0.000 &  -0.021 \\\\\n",
      " congressio &     435 &   17 &    2 &  -0.615 &  -0.623 &  -0.683 &  -0.631 &  -2.514 &  -0.684 &  -0.692 \\\\\n",
      " conn-bench &     208 &   61 &    2 &  -0.087 &  -0.106 &         &  -0.039 &  -0.157 &  -0.000 &  -0.167 \\\\\n",
      " conn-bench &     990 &   12 &   11 &  -1.127 &  -0.040 &  -0.043 &  -0.111 &  -0.052 &  -0.154 &  -0.112 \\\\\n",
      "  connect-4 &   67557 &   43 &    2 &  -0.536 &  -0.398 &         &         &  -0.927 &  -0.389 &  -0.300 \\\\\n",
      "    contrac &    1473 &   10 &    3 &  -0.871 &  -1.221 &         &  -0.691 &  -2.570 &  -0.000 &  -0.494 \\\\\n",
      " credit-app &     690 &   16 &    2 &  -0.363 &  -0.302 &  -0.304 &  -0.328 &  -0.670 &  -0.083 &  -0.302 \\\\\n",
      " cylinder-b &     512 &   36 &    2 &  -0.585 &  -0.479 &  -0.495 &  -0.485 &  -0.894 &  -0.441 &  -0.459 \\\\\n",
      " dermatolog &     366 &   35 &    6 &  -0.183 &  -0.132 &         &  -0.186 &  -0.849 &  -0.152 &  -0.138 \\\\\n",
      " echocardio &     131 &   11 &    2 &  -0.486 &  -0.470 &  -0.476 &  -0.522 &  -2.288 &  -0.743 &  -0.452 \\\\\n",
      "      ecoli &     336 &    8 &    8 &  -0.446 &  -0.154 &  -0.176 &  -0.292 &  -0.994 &  -0.052 &  -0.260 \\\\\n",
      "  energy-y1 &     768 &    9 &    3 &  -0.356 &  -0.297 &  -0.294 &  -0.298 &  -0.160 &  -0.034 &  -0.211 \\\\\n",
      "  energy-y2 &     768 &    9 &    3 &  -0.398 &  -0.507 &         &  -0.325 &  -0.192 &  -0.275 &  -0.246 \\\\\n",
      "  fertility &     100 &   10 &    2 &  -0.160 &         &  -0.135 &  -0.142 &  -0.092 &  -0.012 &  -0.141 \\\\\n",
      "      flags &     194 &   29 &    8 &  -1.519 &  -1.617 &  -1.602 &  -1.384 &  -7.526 &  -2.079 &  -1.787 \\\\\n",
      "      glass &     214 &   10 &    6 &  -0.719 &  -0.179 &  -0.221 &  -0.285 &  -1.518 &  -0.515 &  -0.564 \\\\\n",
      " haberman-s &     306 &    4 &    2 &  -0.517 &  -0.429 &  -0.434 &  -0.444 &  -0.512 &  -0.518 &  -0.445 \\\\\n",
      " hayes-roth &     160 &    4 &    3 &  -1.055 &  -1.062 &  -1.528 &  -0.778 &  -2.269 &  -0.390 &  -0.837 \\\\\n",
      " heart-clev &     303 &   14 &    5 &  -1.138 &  -1.628 &         &  -1.042 &  -4.957 &  -1.463 &  -1.201 \\\\\n",
      " heart-hung &     294 &   13 &    2 &  -0.059 &  -0.038 &  -0.112 &  -0.082 &  -1.142 &  -0.000 &  -0.081 \\\\\n",
      " heart-swit &     123 &   13 &    5 &  -1.494 &  -2.244 &  -2.552 &  -1.565 &  -7.179 &  -2.940 &  -1.948 \\\\\n",
      "   heart-va &     200 &   13 &    5 &  -1.683 &  -2.584 &  -2.807 &  -1.409 &  -7.756 &  -2.431 &  -1.984 \\\\\n",
      "  hepatitis &     155 &   20 &    2 &  -0.498 &  -0.392 &  -0.395 &  -0.399 &  -0.437 &  -0.724 &  -0.415 \\\\\n",
      " hill-valle &    1212 &  101 &    2 &  -0.660 &  -0.695 &         &  -0.697 &  -1.888 &  -0.690 &  -0.701 \\\\\n",
      " horse-coli &     368 &   26 &    2 &  -0.504 &  -0.491 &  -0.501 &  -0.440 &  -1.175 &  -0.409 &  -0.458 \\\\\n",
      " ilpd-india &     583 &   10 &    2 &  -0.632 &  -0.623 &  -0.586 &  -0.580 &  -1.486 &  -0.619 &  -0.701 \\\\\n",
      " image-segm &    2310 &   19 &    7 &  -0.475 &  -0.111 &         &  -0.240 &  -0.201 &  -0.006 &  -0.108 \\\\\n",
      " ionosphere &     351 &   34 &    2 &  -0.307 &  -0.222 &  -0.250 &  -0.129 &  -1.621 &  -0.269 &  -0.233 \\\\\n",
      "       iris &     150 &    5 &    3 &  -0.224 &  -0.005 &  -0.011 &  -0.039 &  -0.015 &  -0.000 &  -0.066 \\\\\n",
      " led-displa &    1000 &    8 &   10 &  -0.919 &  -1.717 &  -1.581 &  -0.914 &  -3.910 &  -0.954 &  -0.853 \\\\\n",
      "     lenses &      24 &    5 &    3 &  -0.774 &         &  -0.911 &  -0.910 &  -0.916 &  -3.917 &  -1.237 \\\\\n",
      "     letter &   20000 &   17 &   26 &  -1.244 &         &         &  -0.207 &  -0.623 &  -0.322 &  -0.176 \\\\\n",
      "     libras &     360 &   91 &   15 &  -0.934 &  -0.460 &         &  -0.755 &  -1.881 &  -0.356 &  -0.617 \\\\\n",
      " low-res-sp &     531 &  101 &    9 &  -0.298 &  -0.164 &         &  -0.280 &  -0.285 &  -0.358 &  -0.272 \\\\\n",
      " lung-cance &      32 &   57 &    3 &  -1.997 &         &  -1.677 &  -1.090 &  -1.090 &  -0.002 &  -2.351 \\\\\n",
      " lymphograp &     148 &   19 &    4 &  -0.317 &  -0.232 &  -0.202 &  -0.307 &  -0.444 &  -0.192 &  -0.284 \\\\\n",
      "      magic &   19020 &   11 &    2 &  -0.020 &  -0.032 &         &  -0.015 &  -0.133 &  -0.000 &  -0.007 \\\\\n",
      " mammograph &     961 &    6 &    2 &  -0.375 &  -0.362 &  -0.359 &  -0.405 &  -0.849 &  -0.345 &  -0.364 \\\\\n",
      "  miniboone &  130064 &   51 &    2 &  -0.010 &  -0.035 &         &  -0.008 &  -0.076 &  -0.001 &  -0.003 \\\\\n",
      " molec-biol &     106 &   58 &    2 &  -0.545 &         &  -0.473 &  -0.667 &  -0.489 &  -0.000 &  -0.692 \\\\\n",
      " molec-biol &    3190 &   61 &    3 &  -0.113 &  -0.051 &         &  -0.091 &  -0.875 &  -0.027 &  -0.131 \\\\\n",
      "    monks-1 &     556 &    7 &    2 &  -0.609 &  -0.126 &  -0.051 &  -0.292 &  -0.322 &  -0.048 &  -0.108 \\\\\n",
      "    monks-2 &     601 &    7 &    2 &  -0.620 &  -0.370 &  -0.484 &  -0.503 &  -0.420 &  -0.589 &  -0.473 \\\\\n",
      "    monks-3 &     554 &    7 &    2 &  -0.405 &  -0.106 &  -0.121 &  -0.108 &  -0.378 &  -0.023 &  -0.069 \\\\\n",
      "   mushroom &    8124 &   22 &    2 &  -0.137 &  -0.014 &         &  -0.000 &  -0.001 &  -0.004 &  -0.002 \\\\\n",
      "     musk-1 &     476 &  167 &    2 &  -0.167 &  -0.131 &         &  -0.091 &  -0.262 &  -0.000 &  -0.179 \\\\\n",
      "     musk-2 &    6598 &  167 &    2 &  -0.006 &  -0.043 &         &  -0.002 &  -0.149 &  -0.000 &  -0.002 \\\\\n",
      "    nursery &   12960 &    9 &    5 &  -0.343 &  -0.257 &  -0.010 &  -0.048 &  -0.139 &  -0.064 &  -0.015 \\\\\n",
      " oocytes\\_me &    1022 &   42 &    2 &  -0.473 &  -0.512 &         &  -0.515 &  -1.561 &  -0.494 &  -0.545 \\\\\n",
      " oocytes\\_me &    1022 &   26 &    3 &  -0.218 &  -0.574 &  -0.353 &  -0.274 &  -0.448 &  -0.330 &  -0.216 \\\\\n",
      " oocytes\\_tr &     912 &   26 &    2 &  -0.546 &  -0.336 &  -0.397 &  -0.382 &  -0.452 &  -0.393 &  -0.410 \\\\\n",
      " oocytes\\_tr &     912 &   33 &    3 &  -0.278 &  -0.298 &  -0.192 &  -0.271 &  -0.763 &  -0.201 &  -0.300 \\\\\n",
      "    optical &    5620 &   63 &   10 &  -0.146 &  -0.056 &         &  -0.046 &  -0.095 &  -0.064 &  -0.043 \\\\\n",
      "      ozone &    2536 &   73 &    2 &  -0.095 &  -0.086 &         &  -0.101 &  -0.489 &  -0.102 &  -0.102 \\\\\n",
      " page-block &    5473 &   11 &    5 &  -0.123 &         &         &  -0.083 &  -0.236 &  -0.083 &  -0.081 \\\\\n",
      " parkinsons &     195 &   23 &    2 &  -0.273 &  -0.108 &  -0.255 &  -0.289 &  -0.162 &  -0.002 &  -0.163 \\\\\n",
      "  pendigits &   10992 &   17 &   10 &  -0.242 &  -0.026 &         &  -0.021 &  -0.069 &  -0.062 &  -0.025 \\\\\n",
      "       pima &     768 &    9 &    2 &  -0.445 &  -0.437 &         &  -0.476 &  -1.499 &  -0.456 &  -0.514 \\\\\n",
      " pittsburg- &     106 &    8 &    3 &  -0.261 &         &  -0.291 &  -0.429 &  -2.552 &  -1.102 &  -0.302 \\\\\n",
      " pittsburg- &     103 &    8 &    3 &  -0.744 &         &  -0.992 &  -0.620 &  -0.654 &  -0.587 &  -0.654 \\\\\n",
      " pittsburg- &      92 &    8 &    3 &  -0.965 &         &  -1.646 &  -0.847 &  -5.926 &  -1.206 &  -1.320 \\\\\n",
      " pittsburg- &     102 &    8 &    2 &  -0.479 &         &  -0.529 &  -0.483 &  -2.745 &  -1.261 &  -0.479 \\\\\n",
      " pittsburg- &     105 &    8 &    6 &  -0.976 &         &  -0.864 &  -1.132 &  -5.616 &  -1.690 &  -1.061 \\\\\n",
      "   planning &     182 &   13 &    2 &  -0.505 &  -0.537 &  -0.577 &  -0.521 &  -1.862 &  -0.678 &  -0.643 \\\\\n",
      " plant-marg &    1600 &   65 &  100 &  -1.366 &         &         &  -1.960 &  -1.143 &  -2.176 &  -0.307 \\\\\n",
      " plant-shap &    1600 &   65 &  100 &  -2.529 &         &         &  -2.182 &  -3.943 &  -1.230 &  -0.934 \\\\\n",
      " plant-text &    1599 &   65 &  100 &  -1.225 &         &         &  -1.925 &  -2.067 &  -2.201 &  -0.605 \\\\\n",
      " post-opera &      90 &    9 &    3 &  -1.009 &         &  -1.947 &  -0.718 &  -3.745 &  -2.205 &  -1.358 \\\\\n",
      " primary-tu &     330 &   18 &   15 &  -1.420 &  -2.333 &  -1.863 &  -1.384 &  -8.060 &  -0.274 &  -1.299 \\\\\n",
      "   ringnorm &    7400 &   21 &    2 &  -0.531 &  -0.071 &         &  -0.028 &  -4.436 &  -0.131 &  -0.050 \\\\\n",
      "      seeds &     210 &    8 &    3 &  -0.064 &  -0.004 &  -0.010 &  -0.038 &  -0.000 &  -0.083 &  -0.047 \\\\\n",
      "    semeion &    1593 &  257 &   10 &  -0.469 &  -0.234 &         &  -0.142 &  -0.518 &  -0.108 &  -0.180 \\\\\n",
      "    soybean &     683 &   36 &   18 &  -0.419 &         &         &  -0.423 &  -1.078 &  -0.088 &  -0.248 \\\\\n",
      "   spambase &    4601 &   58 &    2 &  -0.022 &  -0.018 &         &  -0.050 &  -0.256 &  -0.000 &  -0.032 \\\\\n",
      "      spect &     265 &   23 &    2 &  -0.635 &  -0.676 &  -0.676 &  -0.658 &  -2.663 &  -0.892 &  -0.908 \\\\\n",
      "     spectf &     267 &   45 &    2 &  -0.145 &  -0.077 &  -0.233 &  -0.237 &  -0.190 &  -0.473 &  -0.090 \\\\\n",
      " statlog-au &     690 &   15 &    2 &  -0.628 &  -0.627 &  -0.628 &  -0.623 &  -1.350 &  -0.624 &  -0.661 \\\\\n",
      " statlog-ge &    1000 &   25 &    2 &  -0.490 &  -0.506 &  -0.507 &  -0.562 &  -1.844 &  -0.544 &  -0.729 \\\\\n",
      " statlog-he &     270 &   14 &    2 &  -0.337 &  -0.321 &  -0.322 &  -0.321 &  -1.200 &  -0.392 &  -0.449 \\\\\n",
      " statlog-im &    2310 &   19 &    7 &  -0.346 &         &         &  -0.119 &  -0.277 &  -0.033 &  -0.062 \\\\\n",
      " statlog-la &    6435 &   37 &    6 &  -0.490 &  -0.425 &         &  -0.260 &  -0.472 &  -0.228 &  -0.222 \\\\\n",
      " statlog-sh &   58000 &   10 &    7 &  -0.228 &  -0.016 &         &  -0.009 &  -0.027 &  -0.001 &  -0.004 \\\\\n",
      " statlog-ve &     846 &   19 &    4 &  -0.437 &  -0.718 &  -0.619 &  -0.373 &  -1.103 &  -0.349 &  -0.302 \\\\\n",
      " steel-plat &    1941 &   28 &    7 &  -0.307 &  -0.058 &         &  -0.157 &  -0.663 &  -0.031 &  -0.068 \\\\\n",
      " synthetic- &     600 &   61 &    6 &  -0.102 &  -0.014 &         &  -0.035 &  -0.020 &  -0.031 &  -0.016 \\\\\n",
      "   teaching &     151 &    6 &    3 &  -0.852 &  -1.219 &  -1.467 &  -0.914 &  -0.707 &  -0.667 &  -0.922 \\\\\n",
      "    thyroid &    7200 &   22 &    3 &  -0.139 &  -0.086 &         &  -0.066 &  -0.681 &  -0.005 &  -0.026 \\\\\n",
      " tic-tac-to &     958 &   10 &    2 &  -0.026 &  -0.005 &  -0.021 &  -0.002 &  -0.000 &  -0.000 &  -0.006 \\\\\n",
      "    titanic &    2201 &    4 &    2 &  -0.431 &  -0.019 &  -0.068 &  -0.137 &  -0.000 &  -0.022 &  -0.110 \\\\\n",
      "     trains &      10 &   30 &    2 &  -0.159 &         &  -0.650 &  -2.040 &  -0.511 &  -0.000 &  -0.082 \\\\\n",
      "    twonorm &    7400 &   21 &    2 &  -0.059 &  -0.060 &         &  -0.060 &  -0.155 &  -0.127 &  -0.111 \\\\\n",
      " vertebral- &     310 &    7 &    2 &  -0.033 &  -0.017 &  -0.072 &  -0.012 &  -0.016 &  -0.000 &  -0.013 \\\\\n",
      " vertebral- &     310 &    7 &    3 &  -0.096 &  -0.005 &  -0.056 &  -0.043 &  -0.057 &  -0.000 &  -0.039 \\\\\n",
      " wall-follo &    5456 &   25 &    4 &  -0.713 &         &         &  -0.305 &  -1.134 &  -0.008 &  -0.280 \\\\\n",
      "   waveform &    5000 &   22 &    3 &  -0.315 &  -0.686 &         &  -0.328 &  -0.715 &  -0.328 &  -0.381 \\\\\n",
      " waveform-n &    5000 &   41 &    3 &  -0.342 &  -0.865 &         &  -0.386 &  -0.940 &  -0.366 &  -0.833 \\\\\n",
      "       wine &     178 &   14 &    3 &  -0.066 &  -0.043 &         &  -0.069 &  -0.000 &  -0.044 &  -0.061 \\\\\n",
      " wine-quali &    1599 &   12 &    6 &  -0.968 &  -2.386 &         &  -0.860 &  -2.833 &  -0.905 &  -0.969 \\\\\n",
      " wine-quali &    4898 &   12 &    7 &  -1.075 &         &         &  -0.991 &  -3.135 &  -0.959 &  -0.980 \\\\\n",
      "      yeast &    1484 &    9 &   10 &  -1.057 &         &         &  -0.982 &  -4.304 &  -1.000 &  -0.999 \\\\\n",
      "        zoo &     101 &   17 &    7 &  -0.207 &         &  -0.049 &  -0.230 &  -0.000 &  -0.000 &  -0.016 \\\\\n",
      "        avg &         &      &      &  -0.540 &  -0.450 &  -0.571 &  -0.457 &  -1.370 &  -0.517 &  -0.438 \\\\\n",
      "     median &         &      &      &  -0.446 &  -0.252 &  -0.377 &  -0.314 &  -0.707 &  -0.269 &  -0.300 \\\\\n",
      "   avg rank &         &      &      &   3.066 &   4.826 &   5.273 &   4.041 &   2.149 &   4.512 &   4.132 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(results['test_loglik']['table'], columns=fields).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllll}\n",
      "\\toprule\n",
      "    dataset &       N &    D &    K &    lin &   SVGP &    DGP &    svm &    knn &    gbm &    mlp \\\\\n",
      "\\midrule\n",
      "    abalone &    4177 &    9 &    3 &  0.627 &        &        &  0.667 &  0.624 &  0.696 &  0.660 \\\\\n",
      " acute-infl &     120 &    7 &    2 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      " acute-neph &     120 &    7 &    2 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "      adult &   48842 &   15 &    2 &  0.837 &  0.844 &        &  0.847 &  0.828 &  0.866 &  0.851 \\\\\n",
      "  annealing &     898 &   32 &    5 &  0.767 &  0.800 &        &  0.789 &  0.811 &  0.944 &  0.778 \\\\\n",
      " arrhythmia &     452 &  263 &   13 &  0.609 &  0.696 &        &  0.652 &  0.609 &  0.761 &  0.717 \\\\\n",
      " audiology- &     196 &   60 &   18 &  0.750 &  0.700 &  0.700 &  0.650 &  0.550 &  0.850 &  0.750 \\\\\n",
      " balance-sc &     625 &    5 &    3 &  0.952 &  0.984 &  0.968 &  0.952 &  0.857 &  0.905 &  1.000 \\\\\n",
      "   balloons &      16 &    5 &    2 &  0.000 &        &  0.000 &  0.000 &  0.500 &  0.000 &  0.000 \\\\\n",
      "       bank &    4521 &   17 &    2 &  0.898 &  0.894 &        &  0.894 &  0.898 &  0.909 &  0.892 \\\\\n",
      "      blood &     748 &    5 &    2 &  0.733 &  0.747 &  0.800 &  0.747 &  0.787 &  0.760 &  0.787 \\\\\n",
      " breast-can &     286 &   10 &    2 &  0.897 &  0.931 &  0.966 &  0.966 &  0.724 &  1.000 &  0.966 \\\\\n",
      " breast-can &     699 &   10 &    2 &  0.971 &  0.971 &  0.986 &  0.986 &  0.986 &  0.971 &  1.000 \\\\\n",
      " breast-can &     569 &   31 &    2 &  0.982 &  0.982 &  0.982 &  1.000 &  0.965 &  0.982 &  0.982 \\\\\n",
      " breast-can &     198 &   34 &    2 &  0.600 &  0.700 &  0.700 &  0.600 &  0.600 &  0.700 &  0.700 \\\\\n",
      " breast-tis &     106 &   10 &    6 &  0.818 &        &  1.000 &  0.909 &  0.818 &  0.818 &  0.909 \\\\\n",
      "        car &    1728 &    7 &    4 &  0.821 &  1.000 &  1.000 &  0.983 &  0.936 &  0.960 &  0.988 \\\\\n",
      " cardiotoco &    2126 &   22 &   10 &  0.737 &        &        &  0.803 &  0.718 &  0.878 &  0.840 \\\\\n",
      " cardiotoco &    2126 &   22 &    3 &  0.887 &  0.906 &        &  0.897 &  0.897 &  0.948 &  0.915 \\\\\n",
      " chess-krvk &   28056 &    7 &   18 &  0.583 &        &        &  0.972 &  0.923 &  1.000 &  0.991 \\\\\n",
      " chess-krvk &    3196 &   37 &    2 &  0.984 &  0.997 &        &  0.994 &  0.950 &  1.000 &  0.994 \\\\\n",
      " congressio &     435 &   17 &    2 &  0.682 &  0.682 &  0.568 &  0.682 &  0.545 &  0.636 &  0.545 \\\\\n",
      " conn-bench &     208 &   61 &    2 &  1.000 &  1.000 &        &  1.000 &  0.905 &  1.000 &  0.952 \\\\\n",
      " conn-bench &     990 &   12 &   11 &  0.687 &  1.000 &  1.000 &  0.990 &  0.970 &  0.949 &  1.000 \\\\\n",
      "  connect-4 &   67557 &   43 &    2 &  0.755 &  0.819 &        &        &  0.825 &  0.822 &  0.866 \\\\\n",
      "    contrac &    1473 &   10 &    3 &  0.622 &  0.777 &        &  0.723 &  0.669 &  1.000 &  0.784 \\\\\n",
      " credit-app &     690 &   16 &    2 &  0.841 &  0.855 &  0.855 &  0.841 &  0.841 &  0.986 &  0.884 \\\\\n",
      " cylinder-b &     512 &   36 &    2 &  0.654 &  0.673 &  0.673 &  0.712 &  0.827 &  0.769 &  0.750 \\\\\n",
      " dermatolog &     366 &   35 &    6 &  0.946 &  0.946 &        &  0.946 &  0.919 &  0.946 &  0.946 \\\\\n",
      " echocardio &     131 &   11 &    2 &  0.714 &  0.714 &  0.786 &  0.786 &  0.786 &  0.786 &  0.714 \\\\\n",
      "      ecoli &     336 &    8 &    8 &  0.882 &  0.941 &  0.941 &  0.941 &  0.882 &  0.971 &  0.941 \\\\\n",
      "  energy-y1 &     768 &    9 &    3 &  0.909 &  0.961 &  0.961 &  0.883 &  0.948 &  1.000 &  0.896 \\\\\n",
      "  energy-y2 &     768 &    9 &    3 &  0.844 &  0.922 &        &  0.896 &  0.935 &  0.909 &  0.896 \\\\\n",
      "  fertility &     100 &   10 &    2 &  0.900 &        &  1.000 &  1.000 &  0.900 &  1.000 &  0.900 \\\\\n",
      "      flags &     194 &   29 &    8 &  0.450 &  0.450 &  0.450 &  0.600 &  0.550 &  0.600 &  0.450 \\\\\n",
      "      glass &     214 &   10 &    6 &  0.773 &  0.955 &  0.909 &  0.909 &  0.864 &  0.909 &  0.864 \\\\\n",
      " haberman-s &     306 &    4 &    2 &  0.806 &  0.806 &  0.774 &  0.774 &  0.677 &  0.710 &  0.806 \\\\\n",
      " hayes-roth &     160 &    4 &    3 &  0.375 &  0.625 &  0.625 &  0.750 &  0.625 &  0.812 &  0.562 \\\\\n",
      " heart-clev &     303 &   14 &    5 &  0.581 &  0.581 &        &  0.484 &  0.613 &  0.548 &  0.581 \\\\\n",
      " heart-hung &     294 &   13 &    2 &  1.000 &  1.000 &  0.967 &  0.933 &  0.800 &  1.000 &  0.967 \\\\\n",
      " heart-swit &     123 &   13 &    5 &  0.231 &  0.385 &  0.231 &  0.154 &  0.308 &  0.308 &  0.231 \\\\\n",
      "   heart-va &     200 &   13 &    5 &  0.250 &  0.200 &  0.250 &  0.300 &  0.350 &  0.200 &  0.300 \\\\\n",
      "  hepatitis &     155 &   20 &    2 &  0.688 &  0.750 &  0.750 &  0.812 &  0.750 &  0.750 &  0.750 \\\\\n",
      " hill-valle &    1212 &  101 &    2 &  0.475 &  0.393 &        &  0.393 &  0.484 &  0.541 &  0.426 \\\\\n",
      " horse-coli &     368 &   26 &    2 &  0.730 &  0.757 &  0.730 &  0.757 &  0.757 &  0.730 &  0.784 \\\\\n",
      " ilpd-india &     583 &   10 &    2 &  0.746 &  0.729 &  0.746 &  0.678 &  0.661 &  0.712 &  0.627 \\\\\n",
      " image-segm &    2310 &   19 &    7 &  0.866 &  0.961 &        &  0.900 &  0.961 &  1.000 &  0.957 \\\\\n",
      " ionosphere &     351 &   34 &    2 &  0.833 &  0.806 &  0.833 &  0.944 &  0.889 &  0.944 &  0.861 \\\\\n",
      "       iris &     150 &    5 &    3 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      " led-displa &    1000 &    8 &   10 &  0.710 &  0.750 &  0.730 &  0.710 &  0.750 &  0.700 &  0.750 \\\\\n",
      "     lenses &      24 &    5 &    3 &  0.667 &        &  0.333 &  0.333 &  0.333 &  0.333 &  0.333 \\\\\n",
      "     letter &   20000 &   17 &   26 &  0.712 &        &        &  0.938 &  0.913 &  0.913 &  0.951 \\\\\n",
      "     libras &     360 &   91 &   15 &  0.861 &  0.833 &        &  0.806 &  0.806 &  0.944 &  0.806 \\\\\n",
      " low-res-sp &     531 &  101 &    9 &  0.963 &  0.926 &        &  0.889 &  0.852 &  0.944 &  0.907 \\\\\n",
      " lung-cance &      32 &   57 &    3 &  0.000 &        &  0.250 &  0.000 &  0.500 &  1.000 &  0.000 \\\\\n",
      " lymphograp &     148 &   19 &    4 &  0.867 &  1.000 &  0.933 &  1.000 &  0.800 &  0.867 &  0.867 \\\\\n",
      "      magic &   19020 &   11 &    2 &  0.995 &  0.993 &        &  0.994 &  0.976 &  1.000 &  0.996 \\\\\n",
      " mammograph &     961 &    6 &    2 &  0.845 &  0.825 &  0.825 &  0.804 &  0.845 &  0.845 &  0.835 \\\\\n",
      "  miniboone &  130064 &   51 &    2 &  0.998 &  0.992 &        &  0.997 &  0.990 &  1.000 &  1.000 \\\\\n",
      " molec-biol &     106 &   58 &    2 &  0.727 &        &  0.636 &  0.727 &  0.727 &  1.000 &  0.727 \\\\\n",
      " molec-biol &    3190 &   61 &    3 &  0.975 &  0.984 &        &  0.969 &  0.749 &  0.997 &  0.956 \\\\\n",
      "    monks-1 &     556 &    7 &    2 &  0.661 &  0.964 &  1.000 &  0.893 &  0.911 &  0.982 &  1.000 \\\\\n",
      "    monks-2 &     601 &    7 &    2 &  0.656 &  0.754 &  0.721 &  0.803 &  0.836 &  0.689 &  0.770 \\\\\n",
      "    monks-3 &     554 &    7 &    2 &  0.750 &  0.946 &  0.982 &  0.982 &  0.875 &  1.000 &  1.000 \\\\\n",
      "   mushroom &    8124 &   22 &    2 &  0.947 &  1.000 &        &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "     musk-1 &     476 &  167 &    2 &  0.958 &  0.958 &        &  0.979 &  0.833 &  1.000 &  0.958 \\\\\n",
      "     musk-2 &    6598 &  167 &    2 &  1.000 &  0.989 &        &  1.000 &  0.974 &  1.000 &  1.000 \\\\\n",
      "    nursery &   12960 &    9 &    5 &  0.890 &  0.961 &  0.999 &  0.985 &  0.975 &  0.996 &  0.999 \\\\\n",
      " oocytes\\_me &    1022 &   42 &    2 &  0.806 &  0.786 &        &  0.757 &  0.650 &  0.757 &  0.806 \\\\\n",
      " oocytes\\_me &    1022 &   26 &    3 &  0.883 &  0.903 &  0.883 &  0.883 &  0.883 &  0.864 &  0.922 \\\\\n",
      " oocytes\\_tr &     912 &   26 &    2 &  0.739 &  0.837 &  0.848 &  0.826 &  0.783 &  0.815 &  0.815 \\\\\n",
      " oocytes\\_tr &     912 &   33 &    3 &  0.913 &  0.946 &  0.935 &  0.880 &  0.891 &  0.946 &  0.902 \\\\\n",
      "    optical &    5620 &   63 &   10 &  0.963 &  0.988 &        &  0.995 &  0.982 &  0.979 &  0.980 \\\\\n",
      "      ozone &    2536 &   73 &    2 &  0.972 &  0.969 &        &  0.969 &  0.961 &  0.972 &  0.965 \\\\\n",
      " page-block &    5473 &   11 &    5 &  0.967 &        &        &  0.978 &  0.976 &  0.987 &  0.973 \\\\\n",
      " parkinsons &     195 &   23 &    2 &  0.800 &  0.950 &  0.850 &  0.850 &  0.900 &  1.000 &  0.950 \\\\\n",
      "  pendigits &   10992 &   17 &   10 &  0.945 &  0.995 &        &  0.995 &  0.993 &  0.987 &  0.995 \\\\\n",
      "       pima &     768 &    9 &    2 &  0.831 &  0.805 &        &  0.753 &  0.714 &  0.805 &  0.792 \\\\\n",
      " pittsburg- &     106 &    8 &    3 &  0.909 &        &  0.909 &  0.909 &  0.909 &  0.818 &  0.909 \\\\\n",
      " pittsburg- &     103 &    8 &    3 &  0.636 &        &  0.636 &  0.727 &  0.636 &  0.727 &  0.636 \\\\\n",
      " pittsburg- &      92 &    8 &    3 &  0.700 &        &  0.500 &  0.700 &  0.700 &  0.600 &  0.600 \\\\\n",
      " pittsburg- &     102 &    8 &    2 &  0.727 &        &  0.727 &  0.818 &  0.818 &  0.636 &  0.818 \\\\\n",
      " pittsburg- &     105 &    8 &    6 &  0.727 &        &  0.727 &  0.545 &  0.545 &  0.636 &  0.636 \\\\\n",
      "   planning &     182 &   13 &    2 &  0.789 &  0.789 &  0.737 &  0.789 &  0.684 &  0.579 &  0.632 \\\\\n",
      " plant-marg &    1600 &   65 &  100 &  0.769 &        &        &  0.831 &  0.775 &  0.669 &  0.900 \\\\\n",
      " plant-shap &    1600 &   65 &  100 &  0.588 &        &        &  0.650 &  0.669 &  0.831 &  0.800 \\\\\n",
      " plant-text &    1599 &   65 &  100 &  0.856 &        &        &  0.838 &  0.794 &  0.669 &  0.875 \\\\\n",
      " post-opera &      90 &    9 &    3 &  0.444 &        &  0.444 &  0.556 &  0.556 &  0.333 &  0.444 \\\\\n",
      " primary-tu &     330 &   18 &   15 &  0.545 &  0.576 &  0.576 &  0.515 &  0.515 &  0.970 &  0.606 \\\\\n",
      "   ringnorm &    7400 &   21 &    2 &  0.761 &  0.986 &        &  0.991 &  0.642 &  0.968 &  0.991 \\\\\n",
      "      seeds &     210 &    8 &    3 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "    semeion &    1593 &  257 &   10 &  0.881 &  0.919 &        &  0.963 &  0.919 &  0.969 &  0.938 \\\\\n",
      "    soybean &     683 &   36 &   18 &  0.899 &        &        &  0.913 &  0.826 &  0.986 &  0.884 \\\\\n",
      "   spambase &    4601 &   58 &    2 &  0.996 &  0.993 &        &  0.978 &  0.961 &  1.000 &  0.993 \\\\\n",
      "      spect &     265 &   23 &    2 &  0.667 &  0.630 &  0.630 &  0.704 &  0.556 &  0.481 &  0.519 \\\\\n",
      "     spectf &     267 &   45 &    2 &  1.000 &  0.963 &  0.926 &  0.889 &  0.889 &  0.926 &  1.000 \\\\\n",
      " statlog-au &     690 &   15 &    2 &  0.681 &  0.681 &  0.681 &  0.681 &  0.652 &  0.725 &  0.638 \\\\\n",
      " statlog-ge &    1000 &   25 &    2 &  0.790 &  0.760 &  0.750 &  0.720 &  0.700 &  0.740 &  0.710 \\\\\n",
      " statlog-he &     270 &   14 &    2 &  0.889 &  0.889 &  0.889 &  0.852 &  0.815 &  0.852 &  0.852 \\\\\n",
      " statlog-im &    2310 &   19 &    7 &  0.909 &        &        &  0.957 &  0.905 &  0.983 &  0.983 \\\\\n",
      " statlog-la &    6435 &   37 &    6 &  0.828 &  0.910 &        &  0.915 &  0.922 &  0.927 &  0.905 \\\\\n",
      " statlog-sh &   58000 &   10 &    7 &  0.931 &  0.996 &        &  0.998 &  0.998 &  1.000 &  0.999 \\\\\n",
      " statlog-ve &     846 &   19 &    4 &  0.800 &  0.871 &  0.835 &  0.800 &  0.729 &  0.824 &  0.847 \\\\\n",
      " steel-plat &    1941 &   28 &    7 &  0.944 &  0.990 &        &  0.954 &  0.882 &  0.995 &  0.990 \\\\\n",
      " synthetic- &     600 &   61 &    6 &  1.000 &  1.000 &        &  1.000 &  1.000 &  0.983 &  1.000 \\\\\n",
      "   teaching &     151 &    6 &    3 &  0.750 &  0.625 &  0.688 &  0.625 &  0.688 &  0.875 &  0.562 \\\\\n",
      "    thyroid &    7200 &   22 &    3 &  0.961 &  0.976 &        &  0.978 &  0.954 &  0.999 &  0.992 \\\\\n",
      " tic-tac-to &     958 &   10 &    2 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "    titanic &    2201 &    4 &    2 &  0.814 &  1.000 &  0.995 &  0.946 &  1.000 &  1.000 &  0.995 \\\\\n",
      "     trains &      10 &   30 &    2 &  1.000 &        &  1.000 &  0.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "    twonorm &    7400 &   21 &    2 &  0.978 &  0.978 &        &  0.978 &  0.968 &  0.965 &  0.968 \\\\\n",
      " vertebral- &     310 &    7 &    2 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      " vertebral- &     310 &    7 &    3 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      " wall-follo &    5456 &   25 &    4 &  0.698 &        &        &  0.883 &  0.866 &  0.998 &  0.929 \\\\\n",
      "   waveform &    5000 &   22 &    3 &  0.866 &  0.868 &        &  0.882 &  0.824 &  0.866 &  0.848 \\\\\n",
      " waveform-n &    5000 &   41 &    3 &  0.828 &  0.826 &        &  0.822 &  0.768 &  0.844 &  0.828 \\\\\n",
      "       wine &     178 &   14 &    3 &  1.000 &  1.000 &        &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      " wine-quali &    1599 &   12 &    6 &  0.588 &  0.669 &        &  0.600 &  0.600 &  0.681 &  0.625 \\\\\n",
      " wine-quali &    4898 &   12 &    7 &  0.522 &        &        &  0.590 &  0.563 &  0.602 &  0.567 \\\\\n",
      "      yeast &    1484 &    9 &   10 &  0.597 &        &        &  0.591 &  0.564 &  0.685 &  0.591 \\\\\n",
      "        zoo &     101 &   17 &    7 &  1.000 &        &  1.000 &  1.000 &  1.000 &  1.000 &  1.000 \\\\\n",
      "        avg &         &      &      &  0.793 &  0.859 &  0.790 &  0.816 &  0.805 &  0.853 &  0.826 \\\\\n",
      "     median &         &      &      &  0.828 &  0.924 &  0.842 &  0.883 &  0.833 &  0.944 &  0.896 \\\\\n",
      "   avg rank &         &      &      &  3.025 &  4.707 &  5.405 &  3.678 &  2.893 &  4.409 &  3.884 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(results['test_acc']['table'], columns=fields).to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields = ['dataset', 'N', 'D']\n",
    "\n",
    "                \n",
    "# colours = ['C{}'.format(i) for i in range(10)]\n",
    "\n",
    "# fields = fields + [m[1] for m in models_names]\n",
    "# results = {f:[] for f in fields}\n",
    "\n",
    "\n",
    "# for dataset in regression_datasets:\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#     results['dataset'].append(dataset)\n",
    "#     results['N'].append(ALL_REGRESSION_DATATSETS[dataset].N)\n",
    "#     results['D'].append(ALL_REGRESSION_DATATSETS[dataset].D)\n",
    "\n",
    "#     for (model, name), c in zip(models_names, colours):\n",
    "#         with Database('../results/results.db') as db:\n",
    "#             d = {'model':model, 'dataset':dataset}\n",
    "\n",
    "#             res = db.read('active_learning_continuous', ['total_loglik', 'total_rmse'], d) \n",
    "#         if len(res)>0:\n",
    "#             test_ll = res[0][0]\n",
    "#             test_acc = res[0][1]\n",
    "\n",
    "#             axs[0].plot(test_ll, label=model, color=c)r\n",
    "#             axs[1].plot(test_acc, label=model, color=c)\n",
    "#     axs[0].set_ylim(-10, 10)\n",
    "#     plt.title('{} {} {}'.format(dataset,\n",
    "#                                    ALL_REGRESSION_DATATSETS[dataset].N,\n",
    "#                                    ALL_REGRESSION_DATATSETS[dataset].D))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fields = ['dataset', 'N', 'D', 'K']\n",
    "\n",
    "# models_names = [['linear', 'lin'],\n",
    "#                 ['variationally_sparse_gp', 'SVGP'],\n",
    "#                 ['deep_gp_doubly_stochastic','DGP'],\n",
    "#                 ['svm', 'svm'],\n",
    "#                 ['knn', 'knn'],\n",
    "#                 ['naive_bayes', 'nb'],\n",
    "#                 ['decision_tree', 'dt'],\n",
    "#                 ['random_forest', 'rf'],\n",
    "#                 ['gradient_boosting_machine', 'gbm'],\n",
    "#                 ['adaboost', 'ab'],\n",
    "#                 ['mlp', 'mlp'],\n",
    "#                 ]\n",
    "                \n",
    "# colours = ['C{}'.format(i) for i in range(10)]\n",
    "\n",
    "# fields = fields + [m[1] for m in models_names]\n",
    "# results = {f:[] for f in fields}\n",
    "\n",
    "\n",
    "# for dataset in classification_datasets[:4]:  # don't show them all...\n",
    "    \n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#     results['dataset'].append(dataset)\n",
    "#     results['N'].append(ALL_CLASSIFICATION_DATATSETS[dataset].N)\n",
    "#     results['D'].append(ALL_CLASSIFICATION_DATATSETS[dataset].D)\n",
    "#     results['K'].append(ALL_CLASSIFICATION_DATATSETS[dataset].K)\n",
    "\n",
    "#     for (model, name), c in zip(models_names, colours):\n",
    "#         with Database('../results/results.db') as db:\n",
    "#             d = {'model':model, 'dataset':dataset}\n",
    "\n",
    "#             res = db.read('active_learning_discrete', ['test_loglik', 'total_acc'], d) \n",
    "#         if len(res)>0:\n",
    "#             test_ll = res[0][0]\n",
    "#             test_acc = res[0][1]\n",
    "\n",
    "#             axs[0].plot(test_ll, label=model, color=c)\n",
    "#             axs[1].plot(test_acc, label=model, color=c)\n",
    "\n",
    "#     plt.title('{} {} {} {}'.format(dataset,\n",
    "#                                    ALL_CLASSIFICATION_DATATSETS[dataset].N,\n",
    "#                                    ALL_CLASSIFICATION_DATATSETS[dataset].D,\n",
    "#                                    ALL_CLASSIFICATION_DATATSETS[dataset].K))\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
